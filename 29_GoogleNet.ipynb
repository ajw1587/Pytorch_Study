{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "29_GoogleNet",
      "provenance": [],
      "authorship_tag": "ABX9TyP9a2V6bEJ4mc5HZN3wa2YY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajw1587/Pytorch_Study/blob/main/29_GoogleNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "x3GxayV49IC6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchsummary import summary\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  print('Not GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GoogLeNet Model**\n",
        "GoogLeNet 모델 구축해주기\n",
        "\n",
        "https://www.youtube.com/watch?v=uQc4Fs7yx5I&t=39s\n",
        "\n",
        "https://github.com/Seonghoon-Yu/AI_Paper_Review/blob/master/Classification/GoogLeNet(2014).ipynb"
      ],
      "metadata": {
        "id": "GNN7hn65RcDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GoogLeNet(nn.Module):\n",
        "    def __init__(self,aux_logits=True, num_classes=10, init_weights=True):\n",
        "        super(GoogLeNet, self).__init__()\n",
        "        assert aux_logits == True or aux_logits == False\n",
        "        self.aux_logits = aux_logits\n",
        "\n",
        "        # conv_block takes in_channels, out_channels, kernel_size, stride, padding\n",
        "        # Inception block takes out1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool\n",
        "\n",
        "        self.conv1 = conv_block(3, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.maxpool1 = nn.MaxPool2d(3, 2, 1)\n",
        "        self.conv2 = conv_block(64, 192, kernel_size=3, stride=1, padding=1)\n",
        "        self.maxpool2 = nn.MaxPool2d(3, 2, 1)\n",
        "        self.inception3a = Inception_block(192, 64, 96, 128, 16, 32, 32)\n",
        "        self.inception3b = Inception_block(256, 128, 128, 192, 32, 96, 64)\n",
        "        self.maxpool3 = nn.MaxPool2d(3, 2, 1)\n",
        "        self.inception4a = Inception_block(480, 192, 96, 208, 16, 48, 64)\n",
        "\n",
        "        # auxiliary classifier\n",
        "\n",
        "        self.inception4b = Inception_block(512, 160, 112, 224, 24, 64, 64)\n",
        "        self.inception4c = Inception_block(512, 128, 128, 256, 24, 64, 64)\n",
        "        self.inception4d = Inception_block(512, 112, 144, 288, 32, 64, 64)\n",
        "\n",
        "        # auxiliary classifier\n",
        "\n",
        "        self.inception4e = Inception_block(528, 256, 160, 320, 32, 128, 128)\n",
        "        self.maxpool4 = nn.MaxPool2d(3, 2, 1)\n",
        "        self.inception5a = Inception_block(832, 256, 160, 320, 32, 128, 128)\n",
        "        self.inception5b = Inception_block(832, 384, 192, 384, 48, 128, 128)\n",
        "\n",
        "        self.avgpool = nn.AvgPool2d(7, 1)\n",
        "        self.dropout = nn.Dropout(p=0.4)\n",
        "        self.fc1 = nn.Linear(1024, num_classes)\n",
        "\n",
        "        if self.aux_logits:\n",
        "            self.aux1 = InceptionAux(512, num_classes)\n",
        "            self.aux2 = InceptionAux(528, num_classes)\n",
        "        else:\n",
        "            self.aux1 = self.aux2 = None\n",
        "\n",
        "        # weight initialization\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.inception3a(x)\n",
        "        x = self.inception3b(x)\n",
        "        x = self.maxpool3(x)\n",
        "        x = self.inception4a(x)\n",
        "\n",
        "        if self.aux_logits and self.training:\n",
        "            aux1 = self.aux1(x)\n",
        "\n",
        "        x = self.inception4b(x)\n",
        "        x = self.inception4c(x)\n",
        "        x = self.inception4d(x)\n",
        "\n",
        "        if self.aux_logits and self.training:\n",
        "            aux2 = self.aux2(x)\n",
        "\n",
        "        x = self.inception4e(x)\n",
        "        x = self.maxpool4(x)\n",
        "        x = self.inception5a(x)\n",
        "        x = self.inception5b(x)\n",
        "        x = self.avgpool(x)\n",
        "\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        if self.aux_logits and self.training:\n",
        "            return x, aux1, aux2\n",
        "        else:\n",
        "            return x \n",
        "\n",
        "    # define weight initialization function\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "class conv_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(conv_block, self).__init__()\n",
        "\n",
        "        self.conv_layer = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, **kwargs),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.conv_layer(x)\n",
        "\n",
        "\n",
        "class Inception_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool):\n",
        "        super(Inception_block, self).__init__()\n",
        "\n",
        "        self.branch1 = conv_block(in_channels, out_1x1, kernel_size=1)\n",
        "\n",
        "        self.branch2 = nn.Sequential(\n",
        "            conv_block(in_channels, red_3x3, kernel_size=1),\n",
        "            conv_block(red_3x3, out_3x3, kernel_size=3, padding=1),\n",
        "        )\n",
        "\n",
        "        self.branch3 = nn.Sequential(\n",
        "            conv_block(in_channels, red_5x5, kernel_size=1),\n",
        "            conv_block(red_5x5, out_5x5, kernel_size=5, padding=2),\n",
        "        )\n",
        "\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            conv_block(in_channels, out_1x1pool, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 0차원은 batch이므로 1차원인 filter 수를 기준으로 각 branch의 출력값을 묶어줍니다. \n",
        "        x = torch.cat([self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], 1)\n",
        "        return x\n",
        "\n",
        "# auxiliary classifier의 loss는 0.3이 곱해지고, 최종 loss에 추가합니다. 정규화 효과가 있습니다. \n",
        "class InceptionAux(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(InceptionAux, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=5, stride=3),\n",
        "            conv_block(in_channels, 128, kernel_size=1),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(1024, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "U4-xyXs69Tyh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x = torch.randn(3, 3, 224, 224).to(device)\n",
        "\n",
        "# model = GoogLeNet().to(device)\n",
        "# output = model(x)\n",
        "# print(output)"
      ],
      "metadata": {
        "id": "M8TqCKe9LM1x"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summary(model, input_size=(3, 224, 224), device=device)"
      ],
      "metadata": {
        "id": "lL84nFhJRJHA"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GoogLeNet 학습하기**\n",
        "GoogLeNet 모델 학습해주기"
      ],
      "metadata": {
        "id": "r4tFGcQsR6Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
        "opt = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "lr_scheduler = StepLR(opt, step_size=30, gamma=0.1)\n",
        "\n",
        "def get_lr(opt):\n",
        "    for param_group in opt.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "\n",
        "\n",
        "def metric_batch(output, target):\n",
        "    pred = output.argmax(dim=1, keepdim=True)\n",
        "    corrects = pred.eq(target.view_as(pred)).sum().item()\n",
        "    return corrects\n",
        "\n",
        "\n",
        "\n",
        "def loss_batch(loss_func, outputs, target, opt=None):\n",
        "    if np.shape(outputs)[0] == 3:\n",
        "        output, aux1, aux2 = outputs\n",
        "\n",
        "        output_loss = loss_func(output, target)\n",
        "        aux1_loss = loss_func(aux1, target)\n",
        "        aux2_loss = loss_func(aux2, target)\n",
        "\n",
        "        loss = output_loss + 0.3*(aux1_loss + aux2_loss)\n",
        "        metric_b = metric_batch(output,target)\n",
        "\n",
        "    else:\n",
        "        loss = loss_func(outputs, target)\n",
        "        metric_b = metric_batch(outputs, target)\n",
        "\n",
        "    if opt is not None:\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "    \n",
        "    return loss.item(), metric_b\n",
        "\n",
        "\n",
        "\n",
        "def loss_epoch(model, loss_func, dataset_dl, sanity_check=False, opt=None):\n",
        "    running_loss = 0.0\n",
        "    running_metric = 0.0\n",
        "    len_data = len(dataset_dl.dataset)\n",
        "\n",
        "    for xb, yb in dataset_dl:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        output= model(xb)\n",
        "\n",
        "        loss_b, metric_b = loss_batch(loss_func, output, yb, opt)\n",
        "\n",
        "        running_loss += loss_b\n",
        "\n",
        "        if metric_b is not None:\n",
        "            running_metric += metric_b\n",
        "        \n",
        "        if sanity_check is True:\n",
        "            break\n",
        "\n",
        "    loss = running_loss / len_data\n",
        "    metric = running_metric / len_data\n",
        "\n",
        "    return loss, metric\n",
        "\n",
        "\n",
        "\n",
        "def train_val(model, params):\n",
        "    num_epochs=params[\"num_epochs\"]\n",
        "    loss_func=params[\"loss_func\"]\n",
        "    opt=params[\"optimizer\"]\n",
        "    train_dl=params[\"train_dl\"]\n",
        "    val_dl=params[\"val_dl\"]\n",
        "    sanity_check=params[\"sanity_check\"]\n",
        "    lr_scheduler=params[\"lr_scheduler\"]\n",
        "    path2weights=params[\"path2weights\"]\n",
        "\n",
        "    loss_history = {'train': [], 'val': []}\n",
        "    metric_history = {'train': [], 'val': []}\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    \n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        current_lr = get_lr(opt)\n",
        "        print('Epoch {}/{}, current lr={}'.format(epoch, num_epochs - 1, current_lr))\n",
        "        \n",
        "        model.train()\n",
        "        train_loss, train_metric = loss_epoch(model, loss_func, train_dl, sanity_check, opt)\n",
        "        loss_history['train'].append(train_loss)\n",
        "        metric_history['train'].append(train_metric)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss, val_metric = loss_epoch(model, loss_func, val_dl, sanity_check)\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            torch.save(model.state_dict(), path2weights)\n",
        "            print('Copied best model weights!')\n",
        "\n",
        "        loss_history['val'].append(val_loss)\n",
        "        metric_history['val'].append(val_metric)\n",
        "\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        print('train loss: %.6f, val loss: %.6f, accuracy: %.2f, time: %.4f min' %(train_loss, val_loss, 100*val_metric, (time.time()-start_time)/60))\n",
        "        print('-'*10)\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model, loss_history, metric_history"
      ],
      "metadata": {
        "id": "ciSDpTLXR5wU"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}